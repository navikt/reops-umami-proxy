* Features
** DONE Redact urls
** DONE based on four (actually three) regexes
** Clean Events/Object
*** DONE potential phone IDs removed
*** DONE decide if `device_id` should be redacted
** DONE Path Segments and Query Param
** DONE Add GeoData
** DONE IP-address removal
** DONE Is-Bot ;;
** DONE that kafka-ingress stuff they do

** DONE Forward headers for umami

* Todo
** DONE Deployment
** DONE Probes
*** DONE V- GOES IN README
There's several green threads by tokio running different bits
of the program. They should fail at the same time. Experimentation
with panic! in one service confirms that they do. They also block at the same time
when tested with loop{}.
*** DONE /health

** TODO Grafana dashboard
This one should have
- failure rate
- requests per second
- cpu and memory

** TODO Correlation Ids
Since the requests go through a lifecycle and very concurrent we get
request log message aliasing. These should have a correlation id and color!

** TODO collect-auto
The point of collect-auto is to add api-keys depending on the app-team-environment.
all of these fail without the api-key field.
Imo, the collect-auto bit is a misfeature. (if we're doing a redaction proxy. It's a reasonable thing to have for a
custom event ingestion pipeline )

** DONE log levels

** DONE Structure the code to allow for severeal upstreams :(
Currently we just branch willy-nilly. Perhaps the upstream choice
should get decided in request_filter and put into the context and
then we branch on that. (What are pingora modules? can we use that?)


** TODO Metrics
pingora has built in prometheus
https://grafana.nav.cloud.nais.io/d/ce0c4p51e1tz4f/amplitrude-proxy
** TODO Add metric for cache
** TODO Finish metric for timing of proxy requests
*** TODO Add grafana panel in dashboard for average timing
